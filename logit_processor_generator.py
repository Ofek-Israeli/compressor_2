"""
Generate a standalone SGLang CustomLogitProcessor from compressor_2 artifacts.

Offline (once):
  1. Load fitted KMeans + PCA, deltas JSON, and the LLM tokenizer.
  2. Decode every token ID in the vocab to a string.
  3. Embed all strings, PCA-transform, KMeans-predict → cluster → delta.
  4. Write a .py file with TOKEN_DELTAS baked in (no runtime deps on
     sentence-transformers, sklearn, or compressor_2).

Runtime (SGLang server, every decoding step):
  logits[:, token_id] += delta
"""

from __future__ import annotations

import json
import logging
from datetime import datetime
from pathlib import Path
import numpy as np

logger = logging.getLogger(__name__)

# Number of special clusters prepended by add_special_clusters.py
_SPECIAL_CLUSTER_OFFSET = 2

# ---------------------------------------------------------------------------
# Output template
# ---------------------------------------------------------------------------

_PROCESSOR_TEMPLATE = '''\
"""
Cluster-based logit processor generated by compressor_2.

Generated: {timestamp}
Tokenizer: {llm_tokenizer}
Embedding model: {embedding_model}
KMeans clusters: {n_clusters}
Deltas keys: {deltas_keys}
Vocab size mapped: {vocab_mapped}

Usage with SGLang (extra_body):
    from {module_stem} import LearnedBloatAxisProcessor

    extra_body = {{
        "custom_logit_processor": LearnedBloatAxisProcessor().to_str(),
        "custom_params": LearnedBloatAxisProcessor.get_default_params(),
    }}

Usage with financebench_runner:
    python -m financebench_runner ... --logit-processor {module_stem}.py
"""

import torch
from sglang.srt.sampling.custom_logit_processor import CustomLogitProcessor


class LearnedBloatAxisProcessor(CustomLogitProcessor):
    """SGLang logit processor with baked-in per-token deltas from compressor_2."""

    TOKEN_DELTAS = {token_deltas}

    _delta_tensor = None
    _dt_key = None

    @classmethod
    def get_default_params(cls) -> dict:
        return {{"token_deltas": cls.TOKEN_DELTAS}}

    @classmethod
    def _build_delta_tensor(cls, vocab_size, device, dtype):
        ids = list(cls.TOKEN_DELTAS.keys())
        vals = list(cls.TOKEN_DELTAS.values())
        t = torch.zeros(vocab_size, device=device, dtype=dtype)
        if ids:
            t[torch.tensor(ids, dtype=torch.long, device=device)] = torch.tensor(
                vals, dtype=dtype, device=device,
            )
        cls._delta_tensor = t
        cls._dt_key = (vocab_size, device, dtype)
        return t

    def __call__(self, logits, custom_param_list):
        assert logits.shape[0] == len(custom_param_list)
        key = (logits.shape[1], logits.device, logits.dtype)
        dt = self._delta_tensor
        if dt is None or self._dt_key != key:
            dt = self._build_delta_tensor(*key)
        logits += dt
        return logits
'''


# ---------------------------------------------------------------------------
# Core generator
# ---------------------------------------------------------------------------

def generate_processor(
    kmeans_path: str,
    deltas_path: str,
    output_path: str,
    llm_tokenizer: str,
    embedding_model: str = "all-MiniLM-L6-v2",
    pca_path: str | None = None,
    embeddings_path: str | None = None,
    pca_random_state: int | None = None,
    batch_size: int = 512,
) -> None:
    """Pre-compute token-to-delta mapping and write a standalone .py processor.

    Either *pca_path* (a saved PCA joblib) or *embeddings_path* (the original
    embeddings .npy used to fit PCA) must be provided so that new token
    embeddings can be projected into the same space the KMeans was trained on.
    """
    if pca_path is None and embeddings_path is None:
        raise ValueError("Provide --pca (PCA joblib) or --embeddings (embeddings .npy) to obtain the PCA transform.")

    import joblib

    # ---- load artifacts ----
    kmeans = joblib.load(kmeans_path)
    pca_d = kmeans.cluster_centers_.shape[1]
    logger.info("KMeans: %s clusters, %s-dim centres", kmeans.n_clusters, pca_d)

    with open(deltas_path) as f:
        deltas: dict[str, float] = json.load(f)
    logger.info("Deltas: %s entries  %s", len(deltas), deltas)

    if pca_path is not None:
        pca = joblib.load(pca_path)
        logger.info("PCA loaded from %s (n_components=%s)", pca_path, pca.n_components_)
    else:
        from .pca_reducer import reduce_pca
        X_train = np.load(embeddings_path)
        logger.warning(
            "No PCA joblib provided — re-fitting PCA(d=%s) from %s. "
            "Results are only exact if the same solver/random_state were used originally.",
            pca_d, embeddings_path,
        )
        _, pca = reduce_pca(X_train, d=pca_d, random_state=pca_random_state)

    # ---- load tokenizer & decode vocab ----
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(llm_tokenizer)
    vocab: dict[str, int] = tokenizer.get_vocab()
    logger.info("Tokenizer vocab size: %s", len(vocab))

    eos_ids: set[int] = set()
    if tokenizer.eos_token_id is not None:
        eos_ids.add(tokenizer.eos_token_id)
    eot_id = tokenizer.convert_tokens_to_ids("<|eot_id|>")
    if eot_id is not None and eot_id != tokenizer.unk_token_id:
        eos_ids.add(eot_id)
    logger.info("EOS/EOT token IDs: %s", eos_ids)

    # ---- embed entire vocab ----
    from .embedder import embed_tokens

    all_ids = sorted(vocab.values())
    all_strings = [tokenizer.decode([tid], skip_special_tokens=False) for tid in all_ids]
    logger.info("Embedding %s vocab tokens (batch_size=%s) …", len(all_strings), batch_size)
    X_vocab = embed_tokens(all_strings, model_name=embedding_model, batch_size=batch_size)

    # ---- PCA + KMeans ----
    Z_vocab = pca.transform(X_vocab)
    clusters = kmeans.predict(Z_vocab)
    logger.info("Cluster assignment complete")

    # ---- build delta mapping ----
    eos_delta_str = deltas.get("0", 0.0)
    token_deltas: dict[int, float] = {}
    for idx, tid in enumerate(all_ids):
        if tid in eos_ids:
            d = float(eos_delta_str)
        else:
            cluster = int(clusters[idx])
            key = str(cluster + _SPECIAL_CLUSTER_OFFSET)
            d = float(deltas.get(key, 0.0))
        if d != 0.0:
            token_deltas[tid] = round(d, 6)

    logger.info(
        "Mapped %s tokens to non-zero deltas (out of %s vocab entries)",
        len(token_deltas), len(all_ids),
    )

    # ---- write .py ----
    out = Path(output_path)
    out.parent.mkdir(parents=True, exist_ok=True)
    module_stem = out.stem

    # Format TOKEN_DELTAS as a compact dict literal
    td_str = "{\n" + "".join(
        f"        {tid}: {delta},\n" for tid, delta in sorted(token_deltas.items())
    ) + "    }"

    code = _PROCESSOR_TEMPLATE.format(
        timestamp=datetime.now().isoformat(),
        llm_tokenizer=llm_tokenizer,
        embedding_model=embedding_model,
        n_clusters=kmeans.n_clusters,
        deltas_keys=", ".join(sorted(deltas.keys(), key=int)),
        vocab_mapped=len(token_deltas),
        module_stem=module_stem,
        token_deltas=td_str,
    )

    out.write_text(code)
    logger.info("Wrote %s (%s bytes)", out, len(code))
